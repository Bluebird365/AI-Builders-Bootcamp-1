{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44353ae1-41a5-4e8e-bdfb-fec1f445f239",
   "metadata": {},
   "source": [
    "# Semantic Search with Text Embeddings\n",
    "## ABB #1 - Session 4\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22ee10-b6c8-4e59-babd-366b41f4a357",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c3df73-038b-44b9-9bf2-526dc485c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667ac64-c7f7-4a29-9bf4-9eda692145c8",
   "metadata": {},
   "source": [
    "### 1) chunk articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53d05bc-7ace-455a-a6bd-bdf45116d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files from raw directory\n",
    "filename_list = [\"articles/\"+f for f in os.listdir('articles')]\n",
    "\n",
    "chunk_list = []\n",
    "for filename in filename_list:\n",
    "\n",
    "    # read html file\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Get article title\n",
    "    article_title = soup.find('title').get_text().strip() if soup.find('title') else \"Untitled\"\n",
    "    \n",
    "    # Initialize variables\n",
    "    article_content = []\n",
    "    current_section = \"Main\"  # Default section if no headers found\n",
    "    \n",
    "    # Find all headers and text content\n",
    "    content_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol'])\n",
    "\n",
    "    # iterate through elements and extract text with metadata\n",
    "    for element in content_elements:\n",
    "        if element.name in ['h1', 'h2', 'h3']:\n",
    "            current_section = element.get_text().strip()\n",
    "        elif element.name in ['p', 'ul', 'ol']:\n",
    "            text = element.get_text().strip()\n",
    "            # Only add non-empty content that's at least 30 characters long\n",
    "            if text and len(text) >= 30:\n",
    "                article_content.append({\n",
    "                    'article_title': article_title,\n",
    "                    'section': current_section,\n",
    "                    'text': text\n",
    "                })\n",
    "\n",
    "    # add article content to list\n",
    "    chunk_list.extend(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70b7fbf-c363-4a5a-84b8-be342a450dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chunk list to file\n",
    "filename='data/chunk_list.json'\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50b033-8519-4b93-b8ac-b05eed841604",
   "metadata": {},
   "source": [
    "### 2) embed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e1ddfa-8691-4cb0-8eef-74d7dc7db933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks: 778\n"
     ]
    }
   ],
   "source": [
    "# define text to embed\n",
    "text_list = []\n",
    "for content in chunk_list:\n",
    "    # concatenate title and section header\n",
    "    context = content['article_title'] + \" - \" + content['section'] + \": \"\n",
    "    # append text from paragraph to fill CLIP's 256 sequence limit\n",
    "    text = context + content['text'][:512-len(context)]\n",
    "    \n",
    "    text_list.append(text)\n",
    "print(\"Num chunks:\",len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03fe32a-230a-416a-ac7b-a25d85a41b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# compute embeddings\n",
    "chunk_embeddings = model.encode(text_list)\n",
    "chunk_embeddings.shape\n",
    "\n",
    "# save chunk embeddings to file\n",
    "torch.save(chunk_embeddings, 'data/chunk_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9ac6-134e-421c-904e-ec6bf81834db",
   "metadata": {},
   "source": [
    "### 3) semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e73516-fdfd-4963-ad19-2c3e413ab5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 778])\n",
      "tensor([0.0781, 0.1124, 0.0879, 0.1028, 0.0970])\n"
     ]
    }
   ],
   "source": [
    "# define query\n",
    "query = \"What is a token?\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# compute similarity between query and all chunks\n",
    "similarities = model.similarity(query_embedding, chunk_embeddings)\n",
    "print(similarities.shape)\n",
    "print(similarities[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3aaf616-c522-45c1-a89b-3af6ae4166bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search parameters\n",
    "temp = 0.1\n",
    "k=3\n",
    "threshold = 0.05\n",
    "\n",
    "# Rescale similarities via softmax\n",
    "scores = torch.nn.functional.softmax(similarities/temp, dim=1)\n",
    "\n",
    "# Get sorted indices and scores\n",
    "sorted_indices = scores.argsort(descending=True)[0]\n",
    "sorted_scores = scores[0][sorted_indices]\n",
    "\n",
    "# Filter by threshold and get top k\n",
    "filtered_indices = [\n",
    "    idx.item() for idx, score in zip(sorted_indices, sorted_scores) \n",
    "    if score.item() >= threshold\n",
    "][:k]\n",
    "\n",
    "# Get corresponding content items and scores\n",
    "top_results = [chunk_list[i] for i in filtered_indices]\n",
    "result_scores = [scores[0][i].item() for i in filtered_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d785f-a1df-40aa-b762-2b79f3748df5",
   "metadata": {},
   "source": [
    "### 4) display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "246b8d0f-beea-406b-81b0-484171fb6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_markdown = \"\"\n",
    "for i, result in enumerate(top_results, start=1):\n",
    "    results_markdown += f\"{i}. **Article title:** {result['article_title']}  \\n\"\n",
    "    results_markdown += f\"   **Section:** {result['section']}  \\n\"\n",
    "    results_markdown += f\"   **Snippet:** {result['text']}  \\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97df4231-af86-4dd3-b806-cab95bb4b5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** Cracking Open the OpenAI (Python) API  \n",
       "   **Section:** 2) OpenAI’s (Python) API  \n",
       "   **Snippet:** Tokens, in the context of LLMs, are essentially a set of numbers representing a set of words and characters. For example, “The” could be a token, “ end” (with the space) could be another, and “.” another.  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b101df6f-93a0-4a32-a362-e067397a9140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** We’ve already mentioned situations where RAG and fine-tuning perform well. However, since this is such a common question, it’s worth reemphasizing when each approach works best.  \n",
       "\n",
       "2. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** RAG is when we inject relevant context into an LLM’s input prompt so that it can generate more helpful responses. For example, if we have a domain-specific knowledge base (e.g., internal company documents and emails), we might identify the items most relevant to the user’s query so that an LLM can synthesize information in an accurate and digestible way.  \n",
       "\n",
       "3. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Here’s high-level guidance on when to use each.  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bringing it all together\n",
    "query = \"What's the difference between RAG and Fine-tuning?\"\n",
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=3, threshold=0)\n",
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390287f1-7ea2-4627-a646-e2ff9101b1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
