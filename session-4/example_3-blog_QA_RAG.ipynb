{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c6a7a2-26c9-4e14-aafa-5cf069d02e46",
   "metadata": {},
   "source": [
    "# Article Series QA Assistant with RAG\n",
    "## ABB #1 - Session 4\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff7bfa-ebdb-4323-b16a-d69aa948eb5b",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3ebb26-94ac-4c47-a5fc-170c8caa0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "from functions import *\n",
    "\n",
    "from openai import OpenAI\n",
    "from top_secret import my_sk\n",
    "\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953a58cc-0b6b-4529-a2af-c1f58facd0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup api client\n",
    "client = OpenAI(api_key=my_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe890f-5c07-44c9-8afb-dbc819e82660",
   "metadata": {},
   "source": [
    "### load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029c926b-6f42-4af6-816a-e34dc20aab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chunks\n",
    "filename = 'data/chunk_list.json'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    chunk_list = json.load(f)\n",
    "\n",
    "# load embeddings\n",
    "chunk_embeddings = torch.load('data/chunk_embeddings.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c9b8c5-105a-492e-a14e-fb711e735f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks: 778\n",
      "(778, 384)\n"
     ]
    }
   ],
   "source": [
    "print(\"Num chunks:\",len(chunk_list))\n",
    "print(chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fff575e-faed-4d42-9c19-c4325f1e8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109f511-8897-4e69-aa99-30a2ff79b0c9",
   "metadata": {},
   "source": [
    "### 1) define query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da59e64-ba7b-4150-8615-e23fe9b71a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query\n",
    "query = \"When does it make sense to use RAG vs fine-tuning?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba55143-e771-4603-98a6-80b9f758d26d",
   "metadata": {},
   "source": [
    "### 2) context retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce377b19-c0ec-4519-9cd1-e119911f109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=10, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d16694e7-6dc2-4630-80cb-e1d087e38f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** We’ve already mentioned situations where RAG and fine-tuning perform well. However, since this is such a common question, it’s worth reemphasizing when each approach works best.  \n",
       "\n",
       "2. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Here’s high-level guidance on when to use each.  \n",
       "\n",
       "3. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Previous articles in this series discussed fine-tuning, which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, fine-tuning seems to be less effective than RAG at doing this [1].  \n",
       "\n",
       "4. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** RAG is when we inject relevant context into an LLM’s input prompt so that it can generate more helpful responses. For example, if we have a domain-specific knowledge base (e.g., internal company documents and emails), we might identify the items most relevant to the user’s query so that an LLM can synthesize information in an accurate and digestible way.  \n",
       "\n",
       "5. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Notice that these approaches are not mutually exclusive. In fact, the original RAG system proposed by Facebook researchers used fine-tuning to better use retrieved information for generating responses [4].  \n",
       "\n",
       "6. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Some Nuances  \n",
       "   **Snippet:** Document preparation—The quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well-formatted text file.  \n",
       "\n",
       "7. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Some Nuances  \n",
       "   **Snippet:** While the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated.  \n",
       "\n",
       "8. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When NOT to Fine-tune  \n",
       "   **Snippet:** The effectiveness of any approach will depend on the details of the use case. For example, fine-tuning is less effective than retrieval augmented generation (RAG) to provide LLMs with specialized knowledge [1].  \n",
       "\n",
       "9. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** How it works  \n",
       "   **Snippet:** There are 2 key elements of a RAG system: a retriever and a knowledge base.  \n",
       "\n",
       "10. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Notice that RAG does not fundamentally change how we use an LLM; it's still prompt-in and response-out. RAG simply augments this process (hence the name).  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6442014-564f-4413-9c5f-d2e3b2e55d11",
   "metadata": {},
   "source": [
    "### 3) prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48874ba8-97a1-40ab-9364-90c2c7d96203",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = lambda query, results_markdown : f\"\"\" You are an AI assistant tasked with answering user questions based on excerpts from blog posts. Use the following snippets to \\\n",
    "provide accurate, concise, and synthesized answers. If the snippets don’t provide enough information, let the user know and suggest further exploration.\n",
    "\n",
    "## Question:\n",
    "{query}\n",
    "\n",
    "## Relevant Snippets:\n",
    "{results_markdown}\n",
    "\n",
    "---\n",
    "\n",
    "## Response:\n",
    "Provide a clear and concise response below, synthesizing information from the snippets and referencing them directly. If additional information is \\\n",
    "required, suggest further follow-ups or note what’s missing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6022b678-3522-471f-b8b9-0f896e4eedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template(query, results_markdown)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0551c2a-fde3-4d48-a5d7-d7dfb6f362c4",
   "metadata": {},
   "source": [
    "### 4) prompt GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab389a06-8359-49d1-96db-1663c2fdfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make api call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], \n",
    "    temperature = 0.5\n",
    ")\n",
    "\n",
    "# extract response\n",
    "answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8da58-78a8-4136-8823-40e576d2115b",
   "metadata": {},
   "source": [
    "### 5) display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4dbc7d-88c6-4a4d-9406-a96f16918f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When does it make sense to use RAG vs fine-tuning?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When deciding between Retrieval-Augmented Generation (RAG) and fine-tuning for enhancing large language models (LLMs), consider the following:\n",
       "\n",
       "1. **RAG** is ideal when you need to inject relevant context into the model's input to improve response quality. It works well with domain-specific knowledge bases, allowing the model to synthesize information from relevant documents effectively (Snippet 4). This method is particularly useful when fine-tuning is less effective at providing specialized knowledge (Snippet 8).\n",
       "\n",
       "2. **Fine-tuning** adapts an existing model for a specific use case but is generally considered less effective than RAG for embedding specialized knowledge (Snippet 3). It can be beneficial when you have a well-defined dataset and the goal is to customize the model's behavior more fundamentally.\n",
       "\n",
       "3. Both approaches can be used together; for instance, the original RAG system utilized fine-tuning to enhance how retrieved information is employed in generating responses (Snippet 5).\n",
       "\n",
       "In summary, use RAG when you need to leverage external knowledge sources for better context in responses. Opt for fine-tuning when you want to fundamentally adjust the model's capabilities for a specific task, keeping in mind that it may be less effective for specialized knowledge compared to RAG. If you need more detailed guidance on specific use cases, further exploration of the topic may be beneficial."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print()\n",
    "print(query)\n",
    "print()\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c238da9-3bf7-4333-965c-285be39218b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The benefits of fine-tuning large language models (LLMs) include:\n",
       "\n",
       "1. **Improved Performance for Specific Tasks**: Fine-tuned models can outperform larger pre-trained models for particular use cases, even when clever prompt engineering is applied (Snippet 6).\n",
       "\n",
       "2. **Lower Inference Costs**: Fine-tuning can lead to reduced inference costs, making it a practical choice for deploying AI assistants (Snippet 9).\n",
       "\n",
       "3. **Customization**: Fine-tuning allows for the adaptation of a model to specialized knowledge or tasks, enhancing its relevance and effectiveness (Snippet 2).\n",
       "\n",
       "4. **Quality of Training Data**: The performance of a fine-tuned model is heavily influenced by the quality of the training dataset used, emphasizing the importance of data preparation (Snippet 7).\n",
       "\n",
       "However, it is important to note that fine-tuning is not a one-size-fits-all solution. It may not be as effective as other techniques like retrieval augmented generation (RAG) for certain applications (Snippet 1), and it can incur an \"alignment tax,\" where performance may drop in some tasks (Snippet 5). \n",
       "\n",
       "For further exploration, consider looking into specific use cases where fine-tuning has shown significant benefits or challenges."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bringing it all together\n",
    "query = \"What are the benefits of LLM fine-tuning?\"\n",
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=10, threshold=0.01)\n",
    "answer = answer_query(query, results_markdown, prompt_template, client)\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a2383-fdc7-40ab-a5f6-f4e44cf6d287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
